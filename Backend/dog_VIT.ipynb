{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'archive/dogs.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Select breeds and filter dataframes\n",
    "selected_breeds = ['Afghan', 'Basset', 'Beagle', 'Bearded Collie', 'Bermaise', 'Bloodhound', 'Border Collie', 'Boston Terrier', \n",
    "                   'Boxer', 'Bulldog', 'Chihuahua', 'Chow', 'Corgi', 'Coyote', 'Dalmation','Doberman', 'French Bulldog', 'German Sheperd', \n",
    "                   'Golden Retriever', 'Great Dane', 'Great Perenees', 'Greyhound', 'Irish Spaniel', 'Japanese Spaniel', 'Komondor',\n",
    "                   'Labradoodle', 'Labrador Retriever', 'Lhasa','Malinois', 'Maltese', 'Newfoundland', 'Pekinese', 'Pit Bull', 'Pomeranian',\n",
    "                   'Poodle', 'Pug','Rhodesian', 'Rottweiler', 'Saint Bernard', 'Scotch Terrier','Shiba Inu', 'Shih-Tzu', 'Siberian Husky', 'Vizsla','Yorkie']\n",
    "\n",
    "train_df = df[df['data set'] == 'train']\n",
    "valid_df = df[df['data set'] == 'valid']\n",
    "test_df = df[df['data set'] == 'test']\n",
    "\n",
    "train_df = train_df[train_df['labels'].isin(selected_breeds)]\n",
    "valid_df = valid_df[valid_df['labels'].isin(selected_breeds)]\n",
    "test_df = test_df[test_df['labels'].isin(selected_breeds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rukun\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Modify the DogDataset class to load images using PIL\n",
    "class DogDataset(Dataset):\n",
    "    def __init__(self, dataframe, feature_extractor, transforms=None, root_path=\"archive/\"):\n",
    "        self.dataframe = dataframe\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transforms = transforms\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_path, self.dataframe.iloc[idx]['filepaths'])\n",
    "        label = self.dataframe.iloc[idx]['labels']\n",
    "\n",
    "        # Load image using PIL\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # Use ViTFeatureExtractor\n",
    "        image = self.feature_extractor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n",
    "\n",
    "        return {'input_ids': image, 'labels': label}\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "train_dataset = DogDataset(train_df, feature_extractor)\n",
    "valid_dataset = DogDataset(valid_df, feature_extractor)\n",
    "test_dataset = DogDataset(test_df, feature_extractor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([45]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([45, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=45, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=len(selected_breeds), ignore_mismatched_sizes=True)\n",
    "model.classifier = torch.nn.Linear(in_features=model.config.hidden_size, out_features=len(selected_breeds))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.0, Validation Loss: 4.31528314948082e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   0%|          | 0/1306 [02:39<?, ?it/s, loss=0]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Training Loss: 0.0, Validation Loss: 8.415821503149346e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Training Loss: 0.0, Validation Loss: 9.810443589231e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:   0%|          | 0/1306 [02:32<?, ?it/s, loss=0]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Training Loss: 0.0, Validation Loss: 0.00011431517486926168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Training Loss: 0.0, Validation Loss: 0.00020859450160060078\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loader_iter = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False)\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels_np = LabelEncoder().fit(selected_breeds).transform(batch['labels'])\n",
    "        labels = torch.tensor(labels_np, dtype=torch.long).to(device)  # Convert to PyTorch tensor and move to device\n",
    "        \n",
    "        outputs = model(pixel_values=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loader_iter.set_postfix(loss=loss.item()) \n",
    "    # Validation loop (optional)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels_np = LabelEncoder().fit(selected_breeds).transform(batch['labels'])\n",
    "            labels = torch.tensor(labels_np, dtype=torch.long).to(device)\n",
    "            outputs = model(pixel_values=inputs, labels=labels)\n",
    "            val_loss = outputs.loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_vit_model_45(3)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rukun\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label Index: Yorkie\n",
      "Predicted Probability: 1.00\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = \"fine_tuned_vit_model_45(2)\"  # Change this path to the actual path where you saved the model\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Load the ViT feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Load and preprocess a new image\n",
    "new_image_path = \"archive\\\\test\\\\Yorkie\\\\04.jpg.\"  # Change this path to the path of your new image\n",
    "new_image = Image.open(new_image_path)\n",
    "inputs = feature_extractor(images=new_image, return_tensors='pt')['pixel_values']\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = torch.argmax(logits).item()\n",
    "predicted_probability = probabilities[0, predicted_label].item()\n",
    "\n",
    "print(f\"Predicted Label Index: {selected_breeds[predicted_label]}\")\n",
    "print(f\"Predicted Probability: {predicted_probability:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.90%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "model.to(device)  # Move the model to the GPU\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels_np = LabelEncoder().fit(selected_breeds).transform(batch['labels'])\n",
    "        labels = torch.tensor(labels_np, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(pixel_values=inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'archive/dogs.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Select breeds and filter dataframes\n",
    "selected_breeds = ['Afghan', 'Basset', 'Beagle', 'Bearded Collie', 'Bermaise', 'Bloodhound', 'Border Collie', 'Boston Terrier', \n",
    "                   'Boxer', 'Bulldog', 'Chihuahua', 'Chow', 'Corgi', 'Coyote', 'Doberman', 'French Bulldog', 'German Sheperd', \n",
    "                   'Golden Retriever', 'Great Dane', 'Great Perenees', 'Greyhound', 'Irish Spaniel', 'Japanese Spaniel', 'Komondor',\n",
    "                   'Labradoodle', 'Labrador Retriever', 'Malinois', 'Maltese', 'Newfoundland', 'Pekinese', 'Pit Bull', 'Pomeranian',\n",
    "                   'Poodle', 'Pug', 'Rottweiler', 'Saint Bernard', 'Shiba Inu', 'Shih-Tzu', 'Siberian Husky', 'Yorkie']\n",
    "\n",
    "train_df = df[df['data set'] == 'train']\n",
    "valid_df = df[df['data set'] == 'valid']\n",
    "test_df = df[df['data set'] == 'test']\n",
    "\n",
    "train_df = train_df[train_df['labels'].isin(selected_breeds)]\n",
    "valid_df = valid_df[valid_df['labels'].isin(selected_breeds)]\n",
    "test_df = test_df[test_df['labels'].isin(selected_breeds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rukun\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Modify the DogDataset class to load images using PIL\n",
    "class DogDataset(Dataset):\n",
    "    def __init__(self, dataframe, feature_extractor, transforms=None, root_path=\"archive/\"):\n",
    "        self.dataframe = dataframe\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transforms = transforms\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_path, self.dataframe.iloc[idx]['filepaths'])\n",
    "        label = self.dataframe.iloc[idx]['labels']\n",
    "\n",
    "        # Load image using PIL\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # Use ViTFeatureExtractor\n",
    "        image = self.feature_extractor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n",
    "\n",
    "        return {'input_ids': image, 'labels': label}\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "train_dataset = DogDataset(train_df, feature_extractor)\n",
    "valid_dataset = DogDataset(valid_df, feature_extractor)\n",
    "test_dataset = DogDataset(test_df, feature_extractor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([40]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([40, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=len(selected_breeds), ignore_mismatched_sizes=True)\n",
    "model.classifier = torch.nn.Linear(in_features=model.config.hidden_size, out_features=len(selected_breeds))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/35:   0%|          | 0/1148 [02:13<?, ?it/s, loss=0.249] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35, Training Loss: 0.24925999343395233, Validation Loss: 0.1590338498353958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/35, Training Loss: 0.015759164467453957, Validation Loss: 0.08483875542879105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/35:   0%|          | 0/1148 [02:13<?, ?it/s, loss=0.0135] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/35, Training Loss: 0.013492795638740063, Validation Loss: 0.04303399845957756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/35, Training Loss: 0.004407690372318029, Validation Loss: 0.04036957398056984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/35:   0%|          | 0/1148 [02:13<?, ?it/s, loss=0.00378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/35, Training Loss: 0.0037841994781047106, Validation Loss: 0.01949186623096466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/35, Training Loss: 0.0016689986223354936, Validation Loss: 0.018358057364821434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/35:   0%|          | 0/1148 [02:14<?, ?it/s, loss=0.00164] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/35, Training Loss: 0.0016377372667193413, Validation Loss: 0.015830032527446747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/35, Training Loss: 0.0040533277206122875, Validation Loss: 0.11620575934648514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/35:   0%|          | 0/1148 [02:14<?, ?it/s, loss=0.00396] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/35, Training Loss: 0.003963998984545469, Validation Loss: 0.004260318819433451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/35, Training Loss: 0.000977862160652876, Validation Loss: 0.00397651270031929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/35:   0%|          | 0/1148 [02:14<?, ?it/s, loss=0.000793]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/35, Training Loss: 0.0007932308944873512, Validation Loss: 0.0023873301688581705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/35, Training Loss: 0.0004160024400334805, Validation Loss: 0.0017049345187842846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/35:   0%|          | 0/1148 [02:14<?, ?it/s, loss=0.000116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/35, Training Loss: 0.00011637906573014334, Validation Loss: 0.0011908210581168532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/35, Training Loss: 8.626317139714956e-05, Validation Loss: 0.0008139600977301598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/35:   0%|          | 0/1148 [02:14<?, ?it/s, loss=4.81e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/35, Training Loss: 4.8119487473741174e-05, Validation Loss: 0.0005219408194534481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/35, Training Loss: 4.672890645451844e-05, Validation Loss: 0.00032208123593591154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/35:   0%|          | 0/1148 [02:14<?, ?it/s, loss=1.2e-5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/35, Training Loss: 1.2000317838101182e-05, Validation Loss: 0.00020890652376692742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/35, Training Loss: 1.5934172552078962e-05, Validation Loss: 0.00013393988774623722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/35:   0%|          | 0/1148 [02:13<?, ?it/s, loss=6.2e-6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/35, Training Loss: 6.198863502504537e-06, Validation Loss: 8.503114804625511e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/35, Training Loss: 1.1086402082582936e-05, Validation Loss: 5.225147833698429e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/35:   0%|          | 0/1148 [02:13<?, ?it/s, loss=2.7e-6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/35, Training Loss: 2.7020726065529743e-06, Validation Loss: 3.111292971880175e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/35, Training Loss: 3.5762778338721546e-07, Validation Loss: 1.8556660506874323e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/35:   0%|          | 0/1148 [02:13<?, ?it/s, loss=1.19e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/35, Training Loss: 1.1920920996999484e-06, Validation Loss: 1.0609543096506968e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/35, Training Loss: 1.1126193157906528e-06, Validation Loss: 6.635948466282571e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/35:   0%|          | 0/1148 [02:13<?, ?it/s, loss=1.99e-7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/35, Training Loss: 1.9868211609264108e-07, Validation Loss: 4.331255240686005e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/35, Training Loss: 7.947285496356926e-08, Validation Loss: 3.099433115494321e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/35:   0%|          | 0/1148 [02:13<?, ?it/s, loss=7.95e-8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/35, Training Loss: 7.947285496356926e-08, Validation Loss: 1.9868182334903395e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/35, Training Loss: 3.973642748178463e-08, Validation Loss: 1.6291911606458598e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/35:   0%|          | 0/1148 [02:13<?, ?it/s, loss=0]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/35, Training Loss: 0.0, Validation Loss: 1.3907732636653236e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/35, Training Loss: 7.94728478581419e-08, Validation Loss: 1.4305096556199715e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/35:   0%|          | 0/1148 [02:13<?, ?it/s, loss=0]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/35, Training Loss: 0.0, Validation Loss: 1.6291911606458598e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/35, Training Loss: 0.0, Validation Loss: 1.5894548823780497e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/35:   0%|          | 0/1148 [02:17<?, ?it/s, loss=0]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/35, Training Loss: 0.0, Validation Loss: 2.145762891814229e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/35, Training Loss: 0.0, Validation Loss: 2.46365311795671e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/35:   0%|          | 0/1148 [02:14<?, ?it/s, loss=0]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/35, Training Loss: 0.0, Validation Loss: 3.0994317512522684e-06\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loader_iter = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False)\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels_np = LabelEncoder().fit(selected_breeds).transform(batch['labels'])\n",
    "        labels = torch.tensor(labels_np, dtype=torch.long).to(device)  # Convert to PyTorch tensor and move to device\n",
    "        \n",
    "        outputs = model(pixel_values=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loader_iter.set_postfix(loss=loss.item()) \n",
    "    # Validation loop (optional)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels_np = LabelEncoder().fit(selected_breeds).transform(batch['labels'])\n",
    "            labels = torch.tensor(labels_np, dtype=torch.long).to(device)\n",
    "            outputs = model(pixel_values=inputs, labels=labels)\n",
    "            val_loss = outputs.loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_vit_model_40(2)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rukun\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label Index: Yorkie\n",
      "Predicted Probability: 1.00\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = \"fine_tuned_vit_model_40\"  # Change this path to the actual path where you saved the model\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Load the ViT feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Load and preprocess a new image\n",
    "new_image_path = \"archive\\\\test\\\\Yorkie\\\\04.jpg.\"  # Change this path to the path of your new image\n",
    "new_image = Image.open(new_image_path)\n",
    "inputs = feature_extractor(images=new_image, return_tensors='pt')['pixel_values']\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = torch.argmax(logits).item()\n",
    "predicted_probability = probabilities[0, predicted_label].item()\n",
    "\n",
    "print(f\"Predicted Label Index: {selected_breeds[predicted_label]}\")\n",
    "print(f\"Predicted Probability: {predicted_probability:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.59%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "model.to(device)  # Move the model to the GPU\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels_np = LabelEncoder().fit(selected_breeds).transform(batch['labels'])\n",
    "        labels = torch.tensor(labels_np, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(pixel_values=inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

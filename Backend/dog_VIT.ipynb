{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'archive/dogs.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Select breeds and filter dataframes\n",
    "selected_breeds = ['Afghan', 'Basset', 'Beagle', 'Border Collie', 'Boxer', 'Bulldog', 'Corgi', 'Coyote', 'Doberman',\n",
    "                   'French Bulldog', 'German Sheperd', 'Labradoodle', 'Labrador Retriever', 'Maltese', 'Newfoundland',\n",
    "                   'Pit Bull', 'Pomeranian', 'Poodle', 'Pug', 'Rottweiler', 'Saint Bernard', 'Shiba Inu', 'Shih-Tzu',\n",
    "                   'Siberian Husky', 'Yorkie']\n",
    "\n",
    "train_df = df[df['data set'] == 'train']\n",
    "valid_df = df[df['data set'] == 'valid']\n",
    "test_df = df[df['data set'] == 'test']\n",
    "\n",
    "train_df = train_df[train_df['labels'].isin(selected_breeds)]\n",
    "valid_df = valid_df[valid_df['labels'].isin(selected_breeds)]\n",
    "test_df = test_df[test_df['labels'].isin(selected_breeds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Modify the DogDataset class to load images using PIL\n",
    "class DogDataset(Dataset):\n",
    "    def __init__(self, dataframe, feature_extractor, transforms=None, root_path=\"archive/\"):\n",
    "        self.dataframe = dataframe\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transforms = transforms\n",
    "        self.root_path = root_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_path, self.dataframe.iloc[idx]['filepaths'])\n",
    "        label = self.dataframe.iloc[idx]['labels']\n",
    "\n",
    "        # Load image using PIL\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # Use ViTFeatureExtractor\n",
    "        image = self.feature_extractor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n",
    "\n",
    "        return {'input_ids': image, 'labels': label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rukun\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DogDataset(train_df, feature_extractor)\n",
    "valid_dataset = DogDataset(valid_df, feature_extractor)\n",
    "test_dataset = DogDataset(test_df, feature_extractor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([25]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([25, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=len(selected_breeds), ignore_mismatched_sizes=True)\n",
    "model.classifier = torch.nn.Linear(in_features=model.config.hidden_size, out_features=len(selected_breeds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/734 [01:25<?, ?it/s, loss=0.0203]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.02025425434112549, Validation Loss: 0.374593585729599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Training Loss: 0.0011158408597111702, Validation Loss: 0.01642562821507454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   0%|          | 0/734 [01:27<?, ?it/s, loss=0.000463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Training Loss: 0.00046266347635537386, Validation Loss: 0.05610162019729614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Training Loss: 0.0005157989216968417, Validation Loss: 0.018673105165362358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:   0%|          | 0/734 [01:28<?, ?it/s, loss=0.000248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Training Loss: 0.0002479621907696128, Validation Loss: 0.023597823455929756\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loader_iter = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False)\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels_np = LabelEncoder().fit(selected_breeds).transform(batch['labels'])\n",
    "        labels = torch.tensor(labels_np, dtype=torch.long).to(device)  # Convert to PyTorch tensor and move to device\n",
    "        \n",
    "        outputs = model(pixel_values=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loader_iter.set_postfix(loss=loss.item()) \n",
    "    # Validation loop (optional)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels_np = LabelEncoder().fit(selected_breeds).transform(batch['labels'])\n",
    "            labels = torch.tensor(labels_np, dtype=torch.long).to(device)\n",
    "            outputs = model(pixel_values=inputs, labels=labels)\n",
    "            val_loss = outputs.loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.23%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels_np = LabelEncoder().fit(selected_breeds).transform(batch['labels'])\n",
    "        labels = torch.tensor(labels_np, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(pixel_values=inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = correct_predictions / total_samples\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_vit_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label Index: Siberian Husky\n",
      "Predicted Probability: 1.00\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = \"fine_tuned_vit_model\"  # Change this path to the actual path where you saved the model\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# Load the ViT feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Load and preprocess a new image\n",
    "new_image_path = \"archive\\\\test\\\\Siberian Husky\\\\04.jpg.\"  # Change this path to the path of your new image\n",
    "new_image = Image.open(new_image_path)\n",
    "inputs = feature_extractor(images=new_image, return_tensors='pt')['pixel_values']\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = torch.argmax(logits).item()\n",
    "predicted_probability = probabilities[0, predicted_label].item()\n",
    "\n",
    "print(f\"Predicted Label Index: {selected_breeds[predicted_label]}\")\n",
    "print(f\"Predicted Probability: {predicted_probability:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
